{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9KURqZTRwV6"
      },
      "outputs": [],
      "source": [
        "%pip install trl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_grpo.py\n",
        "from datasets import load_dataset\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "dataset = load_dataset(\"trl-lib/tldr\", split=\"train\")\n",
        "\n",
        "# Define the reward function, which rewards completions that are close to 20 characters\n",
        "def reward_len(completions, **kwargs):\n",
        "    return [-abs(20 - len(completion)) for completion in completions]\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    output_dir=\"Qwen2-0.5B-GRPO\",\n",
        "    per_device_train_batch_size=2,\n",
        "    learning_rate=5e-6,\n",
        "    num_generations=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    logging_steps=5,\n",
        "    save_steps=50,\n",
        "    max_steps=75,\n",
        ")\n",
        "trainer = GRPOTrainer(\n",
        "    model=\"Qwen/Qwen2-0.5B-Instruct\",\n",
        "    reward_funcs=reward_len,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "0Pk9yKohSpgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"Qwen2-0.5B-GRPO-final\")"
      ],
      "metadata": {
        "id": "UpCOI945TQtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inference_len.py\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load your fine-tuned model\n",
        "model_path = \"Qwen2-0.5B-GRPO-final\"  # or \"Qwen2-0.5B-GRPO-final\" if you saved with that name\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\")\n",
        "\n",
        "def generate(prompt, max_new_tokens=50):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Example: test with your Reddit story\n",
        "reddit_story =\"Summarize: I went to the store and bought some milk, eggs, and bread.But i kept the extra money so my mom slapped me.\"\n",
        "\n",
        "prompt = f\"Write something about this:\\n\\n{reddit_story}\\n\\nOutput:\"\n",
        "print(generate(prompt))"
      ],
      "metadata": {
        "id": "hYYxhzo8ajms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pFI4ZDJubdpp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}